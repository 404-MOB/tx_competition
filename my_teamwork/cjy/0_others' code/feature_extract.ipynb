{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import gc\n",
    "import sys\n",
    "from scipy import sparse\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['age', 'gender', 'education', 'consumptionAbility', 'LBS', 'carrier', 'house']\n",
    "\n",
    "multi_categorical_features = ['marriageStatus', 'ct', 'os']\n",
    "\n",
    "tfidf_features = ['interest1', 'interest2', 'interest3', 'interest4', 'interest5', 'kw1', 'kw2', 'kw3', 'topic1','topic2', 'topic3']\n",
    "\n",
    "id_features = ['creativeId', 'advertiserId', 'campaignId', 'adCategoryId', 'creativeSize', 'productId', 'productType']\n",
    "\n",
    "user_action_features = ['appIdInstall', 'appIdAction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical2vector(columns, column_names=None):\n",
    "    \"\"\"\n",
    "    convert categorical ID feature to discrete vector(one hot)\n",
    "    :param columns: pandas Series or pandas DataFrame\n",
    "    :param column_names: column list name\n",
    "    :return: ndarray shape is (num_samples,classes)\n",
    "    \"\"\"\n",
    "    enc = OneHotEncoder()\n",
    "    labelEnc = LabelEncoder()\n",
    "    if isinstance(columns, pd.DataFrame) and column_names is not None:\n",
    "        columns.fillna(0, inplace=True)  # handle missing value fill with 0 or -1?\n",
    "        # return enc.fit_transform(labelEnc.fit_transform(columns)).toarray()\n",
    "        a = np.zeros(shape=(len(columns), 1))\n",
    "        for column_name in column_names:\n",
    "            label_array = labelEnc.fit_transform(columns[column_name]).reshape(-1, 1)\n",
    "            a = np.concatenate((a, label_array), axis=1)\n",
    "        a = np.delete(a, 0, axis=1)\n",
    "        return enc.fit_transform(a).toarray()\n",
    "    elif isinstance(columns, pd.Series):\n",
    "        columns.fillna(0, inplace=True)\n",
    "        # return labelEnc.fit_transform(columns.values).reshape(1,-1)\n",
    "        return enc.fit_transform(labelEnc.fit_transform(columns.values).reshape(-1, 1)).toarray()\n",
    "    else:\n",
    "        raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multicategorical2vector(columns, column_names=None):\n",
    "    \"\"\"\n",
    "    convert multi-categorical ID feature to discrete vector\n",
    "    :param columns: pandas Series\n",
    "    :param column_names: column list name\n",
    "    :return: ndarray shape is (num_samples,classes)\n",
    "    \"\"\"\n",
    "    columns.fillna(0, inplace=True)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    if isinstance(columns, pd.DataFrame) and column_names is not None:\n",
    "        a = None\n",
    "        for column_name in column_names:\n",
    "            column = columns[column_name]\n",
    "            column = column.apply(lambda x: x.split())\n",
    "            if a is None:\n",
    "                a = mlb.fit_transform(column.values)\n",
    "                print('column name is ' + column_name)\n",
    "                print('a shape is ' + str(a.shape))\n",
    "                print('size of vector a is ' + str(sys.getsizeof(a) / (1024 * 1024)))\n",
    "            else:\n",
    "                tmp = mlb.fit_transform(column.values)\n",
    "                a = np.concatenate((a, tmp), axis=1)\n",
    "                print('column name is ' + column_name)\n",
    "                print('a shape is ' + str(a.shape))\n",
    "                print('size of vector a is ' + str(sys.getsizeof(a) / (1024 * 1024)))\n",
    "                del tmp\n",
    "                gc.collect()\n",
    "        return a\n",
    "    elif isinstance(columns, pd.Series):\n",
    "        print(columns.value_counts())\n",
    "        columns = columns.apply(lambda x: x.split())\n",
    "        return mlb.fit_transform(columns.values)\n",
    "    else:\n",
    "        raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categorical_features(df_train, categorical_features):\n",
    "    \"\"\"\n",
    "    extract categorical features and store as csv file\n",
    "    :param df_train: source file\n",
    "    :param categorical_features: categorical features list\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    cat_mat = categorical2vector(df_train[categorical_features], categorical_features)\n",
    "    df_cat = pd.DataFrame(data=cat_mat, dtype='int8')\n",
    "    # df_cat.to_csv('../input/train_categorical_features.csv', index=False, encoding='utf-8')\n",
    "    return df_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_features(df_train, id_features):\n",
    "    \"\"\"\n",
    "    use label encoder to encode id features\n",
    "    :param df_train:\n",
    "    :param id_features:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    lb = LabelEncoder()\n",
    "    a = np.zeros(shape=(len(df_train), 1))\n",
    "    for _id in id_features:\n",
    "        column = df_train[_id]\n",
    "        column = lb.fit_transform(column).reshape(-1, 1)\n",
    "        a = np.concatenate((a, column), axis=1)\n",
    "    a = np.delete(a, 0, axis=1)\n",
    "    df_id = pd.DataFrame(data=a, dtype='int16')\n",
    "    # df_id.to_csv('../input/train_id_features.csv', index=False, encoding='utf-8')\n",
    "    return df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_multicategorical_features(df_train, multicategorical_features):\n",
    "    \"\"\"\n",
    "    use multilabelbinarizer to encode features\n",
    "    :param df_train:\n",
    "    :param multicategorical_features:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    multi_mat = multicategorical2vector(df_train[multicategorical_features], multicategorical_features)\n",
    "    df_multi = pd.DataFrame(data=multi_mat, dtype='int8')\n",
    "    # df_multi.to_csv('../input/train_multi_categorical_features.csv', index=False, encoding='utf-8')\n",
    "    return df_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tfidf_features(column):\n",
    "    \"\"\"\n",
    "    extract tf-idf features global\n",
    "    :param column:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tfidfVec = TfidfVectorizer(\n",
    "        ngram_range=(1, 1),\n",
    "        analyzer='word',\n",
    "        min_df=3000,\n",
    "    )\n",
    "    return tfidfVec.fit_transform(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tfidf_features_by_aid(df, column):\n",
    "    \"\"\"\n",
    "    extract tf-idf features according to each different aid local tf-idf\n",
    "    :param df: data frame\n",
    "    :param column: pandas Series\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    aids = df['aid'].unique()\n",
    "    # construct vocabulary\n",
    "    # vocabulary = list()\n",
    "    # df[column].apply(lambda x: vocabulary.extend(x.split()))\n",
    "    # vocabulary = set(vocabulary)\n",
    "    # print('vocabulary size is {}'.format(len(vocabulary)))\n",
    "    # print(vocabulary)\n",
    "    tfidfVec = TfidfVectorizer(\n",
    "        ngram_range=(1, 1),\n",
    "        analyzer='word',\n",
    "        min_df=3000,\n",
    "        # vocabulary=vocabulary\n",
    "    )\n",
    "    tfidf_mat = None\n",
    "    for aid in aids:\n",
    "        sub_df = df[df['aid'] == aid]\n",
    "        print('sub data frame shape is {}'.format(sub_df.shape))\n",
    "        tmp = tfidfVec.fit_transform(sub_df[column])\n",
    "        print(tmp.shape)\n",
    "        if tfidf_mat is None:\n",
    "            tfidf_mat = tmp\n",
    "        else:\n",
    "            if tmp.shape[1] > tfidf_mat.shape[1]:\n",
    "                tfidf_mat = sparse.hstack(\n",
    "                    (tfidf_mat, np.zeros(shape=(tfidf_mat.shape[0], tmp.shape[1] - tfidf_mat.shape[1]))))\n",
    "            elif tmp.shape[1] < tfidf_mat.shape[1]:\n",
    "                tmp = sparse.hstack(\n",
    "                    (tmp, np.zeros(shape=(tmp.shape[0], tfidf_mat.shape[1] - tmp.shape[1]))))\n",
    "            else:\n",
    "                pass\n",
    "            tfidf_mat = sparse.vstack((tfidf_mat, tmp))\n",
    "            print('tfidf matrix shape is {}'.format(tfidf_mat.shape))\n",
    "        del tmp\n",
    "        gc.collect()\n",
    "    return tfidf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probability_features(df, column_name, df_ad):\n",
    "    \"\"\"\n",
    "    extract the global positive probability for every single feature eg.uid , ad features...\n",
    "    :param df:\n",
    "    :param column_name:\n",
    "    :param df_ad\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df_positive = df[df['label'] == '1']\n",
    "    print(len(df_positive))\n",
    "    column_value_count = df[column_name].value_counts()\n",
    "    result = []\n",
    "    for index, item in df_ad.iterrows():\n",
    "        print('handing line {}'.format(index))\n",
    "        # total = len(df[df[column_name] == item[column_name]])\n",
    "        result_dict = dict()\n",
    "        for value in df[column_name].unique():\n",
    "            positive_count = len(df_positive[df_positive[column_name] == value])\n",
    "            total = column_value_count[value]\n",
    "            positive_rate = round(positive_count / total, 8) if total != 0 else 0\n",
    "            # print(positive_count)\n",
    "            result_dict[value] = positive_rate\n",
    "        result.append(result_dict)\n",
    "    df_statics = pd.DataFrame(data=result, columns=df[column_name].unique(), dtype='float16')\n",
    "    df_statics['aid'] = df_ad['aid']\n",
    "    df_statics.to_csv('../input/statics/statics_' + column_name + '.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probability_features_each_aid(df_ad=None, column_name=None, df_train=None, df_positive=None):\n",
    "    \"\"\"\n",
    "    extract each positive probability for each aid of a single-value column eg. user features\n",
    "    :param df_ad:\n",
    "    :param column_name:\n",
    "    :param df_train:\n",
    "    :param df_positive:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # aid = row[0]\n",
    "    # positive_df = df_positive[df_positive['aid'] == aid]\n",
    "    # positive_count = len(positive_df[positive_df[column_name] == row[target_index]])\n",
    "    # aid_value_count = df_train['aid'].value_counts()[row[0]]\n",
    "    # print('go go')\n",
    "    # if aid_value_count == 0:\n",
    "    #     return 0\n",
    "    # else:\n",
    "    #     return positive_count / aid_value_count\n",
    "    result = []\n",
    "    print('start calculating {} statics'.format(column_name))\n",
    "    for index, item in df_ad.iterrows():\n",
    "        print('calculating {}'.format(index))\n",
    "        positive_df = df_positive[df_positive['aid'] == str(item['aid'])]\n",
    "        total = len(df_train[df_train['aid'] == str(item['aid'])])\n",
    "        result_dict = dict()\n",
    "        for value in df_train[column_name].unique():\n",
    "            positive_count = len(positive_df[positive_df[column_name] == value])\n",
    "            if total == 0:\n",
    "                result_dict[value] = 0\n",
    "            else:\n",
    "                result_dict[value] = round(positive_count / total, 8)\n",
    "            print('{} positive rate is {}'.format(value, result_dict[value]))\n",
    "        result.append(result_dict)\n",
    "    df_statics = pd.DataFrame(data=result, columns=df_train[column_name].unique(), dtype='float16')\n",
    "    df_statics['aid'] = df_ad['aid']\n",
    "    df_statics.to_csv('../input/statics/statics_' + column_name + '.csv', index=False, encoding='utf-8')\n",
    "    return df_statics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probability_features_each_aid_multi(df_ad, df_train, column_name):\n",
    "    \"\"\"\n",
    "    extract each positive probability for each aid of a multi-value column eg. user features-interest ..topic..\n",
    "    :param df_ad:\n",
    "    :param df_train:\n",
    "    :param column_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    df_positive = df_train[df_train['label'] == '1']\n",
    "    result = []\n",
    "    value_list = []\n",
    "    for value in df_train[column_name].unique():\n",
    "        value_list.extend(value.split())\n",
    "    value_set = set(value_list)\n",
    "    for index, item in df_ad.iterrows():\n",
    "        print('calculating index {}'.format(index))\n",
    "        positive_df = df_positive[df_positive['aid'] == str(item['aid'])]\n",
    "        total = len(df_train[df_train['aid'] == str(item['aid'])])\n",
    "        result_dict = dict()\n",
    "\n",
    "        value_count = positive_df[column_name].value_counts()\n",
    "        for value in value_set:\n",
    "            for value_count_index in value_count.index:\n",
    "                if value in value_count_index.split():\n",
    "                    result_dict[value] = result_dict.get(value, 0) + value_count[value_count_index]\n",
    "\n",
    "        # for index, item in positive_df.iterrows():\n",
    "        #     for value in value_set:\n",
    "        #         if value in item[column_name].split():\n",
    "        #             result_dict[value] = result_dict.get(value, 0) + 1\n",
    "        for value in value_set:\n",
    "            result_dict[value] = round(result_dict.get(value, 0) / total, 8) if total != 0 else 0\n",
    "            print('{} positive rate is {}'.format(value, result_dict[value]))\n",
    "        result.append(result_dict)\n",
    "    df = pd.DataFrame(data=result, columns=list(value_set))\n",
    "    df['aid'] = df_ad['aid']\n",
    "    df.to_csv('../input/statics/statics_' + column_name + '.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_max_probability_each_aid_multi(row, df_statics, column_index):\n",
    "    \"\"\"\n",
    "    extract the max probability in each multi-value column like max-pooling\n",
    "    :param row:\n",
    "    :param df_statics:\n",
    "    :param column_index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    aid = row[0]  # str\n",
    "    column_values = row[column_index].split()\n",
    "    df_statics = df_statics[df_statics['aid'] == int(aid)]\n",
    "    # print(df_statics.head())\n",
    "    max_value = df_statics[column_values].max()\n",
    "    print('max value is {}'.format(max_value[0]))\n",
    "    return max_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_positive_probability_single(row, df_statics, column_index):\n",
    "    \"\"\"\n",
    "    extract single-value positive probability in df_statics file\n",
    "    :param row:\n",
    "    :param df_statics:\n",
    "    :param column_index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    aid = row[0]\n",
    "    column_value = row[column_index]\n",
    "    df_statics = df_statics[df_statics['aid'] == int(aid)]\n",
    "    try:\n",
    "        return df_statics[column_value]\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train_clean.csv', encoding='utf-8', dtype=object)\n",
    "    # df_test = pd.read_csv('../input/test_clean.csv', encoding='utf-8', dtype=object)\n",
    "    # print(df_train['aid'].value_counts())\n",
    "    # print(df_train['LBS'].value_counts())\n",
    "    # print(multicategorical2vector(df_train[multi_categorical_features], column_names=multi_categorical_features))\n",
    "\n",
    "    # extract categorical features\n",
    "    # df = extract_categorical_features(df_test, categorical_features)\n",
    "    # df.to_csv('../input/test_categorical_features.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    # extract id features\n",
    "    # df = extract_id_features(df_test, id_features)\n",
    "    # df.to_csv('../input/test_id_features.csv', encoding='utf-8', index=False)\n",
    "\n",
    "    # extract multi-categorical features\n",
    "    # df = extract_multicategorical_features(df_test, multi_categorical_features)\n",
    "    # df.to_csv('../input/test_multi_categorical_features.csv', encoding='utf-8', index=False)\n",
    "    # mat = extract_tfidf_features_by_aid(df_train, 'interest1')\n",
    "\n",
    "    # build statics features\n",
    "    df_ad = pd.read_csv('../input/adFeature.csv', encoding='utf-8')\n",
    "    df_positive = df_train[df_train['label'] == '1']\n",
    "    print(len(df_positive))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single value group by aid\n",
    "    # for feature in ['gender', 'education', 'consumptionAbility', 'LBS', 'carrier', 'house', 'age']:\n",
    "    #     try:\n",
    "    #         extract_probability_features_each_aid(df_ad=df_ad, column_name=feature, df_train=df_train,\n",
    "    #                                               df_positive=df_positive)\n",
    "    #     except Exception as e:\n",
    "    #         continue\n",
    "\n",
    "    # single value\n",
    "    # for feature in in['advertiserId', 'campaignId', 'adCategoryId', 'creativeSize', 'productId', 'productType']:\n",
    "    #     try:\n",
    "    #         extract_probability_features(df_train, feature, df_ad)\n",
    "    #     except Exception as e:\n",
    "    #         continue\n",
    "\n",
    "    # for feature in ['interest1', 'interest2', 'interest3', 'interest4', 'interest5', 'kw1', 'kw2', 'kw3', 'topic1',\n",
    "    #                 'topic2', 'topic3', 'appIdInstall', 'appIdAction']:\n",
    "    # for feature in ['marriageStatus', 'creativeId', 'ct', 'os', 'kw2', 'kw3', 'topic1', 'topic2', 'topic3',\n",
    "    #                 'appIdInstall', 'appIdAction', 'kw1']:\n",
    "    # for feature in ['kw2', 'kw3', 'topic1', 'topic2', 'topic3', 'appIdInstall', 'appIdAction', 'kw1']:\n",
    "    #     try:\n",
    "    #         extract_probability_features_each_aid_multi(df_ad=df_ad, df_train=df_train, column_name=feature)\n",
    "    #     except Exception as e:\n",
    "    #         print(e)\n",
    "    # continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
